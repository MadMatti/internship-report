\documentclass[12pt, a4paper, oneside]{article}


\usepackage[utf8]{inputenc}
\usepackage[hidelinks]{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    filecolor=magenta,
    urlcolor=blue,
    pdftitle={Internship Report},
    pdfpagemode=FullScreen,
}
\usepackage[none]{hyphenat}
\usepackage{pythonhighlight}
\usepackage{fancyvrb}
\usepackage{verbatim}
\usepackage{varwidth}
\usepackage{graphicx}
\graphicspath{{./images/}}


\title{\huge \textbf{Internship Report}}
\author{Mattia Evangelisti \\ s268637}
\date{June 2022}


\begin{document}

\begin{titlepage}
    \maketitle
    \begin{center}
        An internship report presented for the degree of\\
        Computer Engineering    
    \end{center}
\end{titlepage}

\tableofcontents
\clearpage\null\newpage


\newpage
\section{Summary}
At the beginning of the academic year I chose to substitute my $3^{rd}$ free ECTS exam with an internship. For my study programme was slated a 10 credits internship. Since I decided to work part-time
for the whole experience, it last 3 months.\\
During my internship experience with Ubroker, I have developed my own data mining project from scratch, developing the necessaries skills and knowledge.\\
The main objective of the project was to retrieve data about employees, company devices and their usage, and to create reports that would be useful for the company.\\
The project was developed in Python, language that I learned thanks to the constant support of my colleagues.\\
The key points of the project could be summarized as follows: data retrieval from Microsoft Azure, through the use of Microsoft Graph APIs, data manipulation, data storage in an internal database,
and the creation of reports.\\
Although I found the project to be a challenging experience, I found it to be valuable in developing my skills in Python programming, data manipulation and database management.\\


\newpage
\section{Company}
Ubroker srl was founded in 2015 with the goal of providing eco-friendly energy to their clients, without neglecting the need of a competitive price.\\
More information can be found in their \href{https://ubroker.it/}{website}.
\subsection{Mission}
The company's mission is to improve the life quality of its clients by granting them a simple management system that allows them to control their energy consumption and a cost saving service, providing 
competitive prices below the market average. Moreover, Ubroker is constantly striving to maintain the social responsibility by choosing providers who follow policies that safeguard the environment and cultures
involved in the energy distribution process.

\subsection{Market Strategy}
Ubroker is an electricity and gas provider, that collocate itself at the last link of the energy distribution chain. Their working area is indeed the sales to the final client.\\
The company sales target is the domestic market, the majority of their customers are private citizens. The most recent data report 85,000 active clients, who subscribed 52,000 electricity contracts
and 32,000 gas contracts.\\
The peculiarity of Ubroker's market strategy is the system called \emph{scelgo zero}. Each client can join the zero project though their \href{https://scelgozero.it/}{platform} and start to reduce to zero their
electricity and gas bill.\\
On their website, each client may have two options: to become a testimonial and start to zero their bills and the possibility to become an advisor of the zero project. The first option allows the clients to reduce their bill, indeed each user will receive points
every time they invite a new client and every time an invited user bring in someone else. Through this mechanism, the bill will be proportionally reduced, possibly until zero.
On the other hand, by choosing the second options, a user may become an external advisor, whose role will be to sell contracts to new potential clients, earning a commission for each new subscription. 
Moreover, those external partners will have access to a wide range of training courses, that can be chosen and followed on the platform.

\newpage
\subsection{Economic Growth}
Ubroker was founded in 2015 and since then, thanks to the innovative market strategy and the strong digitalization of all the services, the company has been able to grow rapidly. Thanks to those characteristics,
they were able to achieve incredible goals, such as a total of 170000 clients and over 2 million issued invoices. Moreover, the company has recently gained a Microsoft partnership.

\subsection{Working environment}
The company currently employs about 50 people, divided in various departments, such as sales, marketing, IT, legal, accounting, etc.\\
The main office is located in the city of Collegno, in the metropolitan area of Turin and, at the moment is composed of one building, but they will be able to expand their space in a second build, within the next
few months. Moreover, the company is still granting to the employees the possibility of working remotely, through the use of devices provided by the company itself.

\subsection{IT department}
Being a computer engineering student, I spent most of my internship in the IT department, which is composed of a team of 8 people. The main goal of this department is the software development, 
for both internal and external use.\\ 
The team covers all the aspects of a software life, from the development of the software itself, including both front-end and back-end development, to the integration and the maintenance of the software with the company's 
infrastructure.\\
The main projects of the IT department are: \emph{Piattaforma Zero}, which was described before, several web applications used by the other departments and an internal system for the management of the company's contracts.\\
All the software are developed using the Docker technology, which allows to create a containerized environment, in which the software can be developed, installed and run. The front-end developing is mostly done
using the JavaScript framework Angular, on the other hand, the back-end developers deals with PHP framework Laminas and PostgreSQL databases.\\
Moreover, I found fascinating how the whole infrastructure is cloud based. The company make indeed an intense use the Azure cloud, which is a cloud computing platform owned by Microsoft, that allows
to have storage and virtual machines in the cloud. Ubroker uses about 60 virtual machines and over 6 TB of storage on the Azure platform. Thanks to this intense use of this service, the company is recently
become a Microsoft partner.\\
My role in the department was to create a software able to retrieve data about employees, company devices and their usage, and to create reports that would be useful for the company.

\newpage
\section{Project Introduction}
The project can be summarized in 4 main parts:
\begin{itemize}
    \item Data retrieval from Microsoft Azure, through the use of Microsoft Graph APIs.
    \item Data cleaning and manipulation.
    \item Data storage in an internal database.
    \item Report creation using Power Bi.
\end{itemize}
\subsection{Used technologies}
The project was entirely written in Python 3.9 programming language and the following libraries were used:
\begin{verbatim}
    colorama==0.4.4
    jsonschema==4.4.0
    msal==1.17.0
    pandas==1.4.2   
    psycopg2==2.9.3
    requests==2.27.1
    tabulate==0.8.9
\end{verbatim}
For the database management it was chosen to use PostgreSQL, as long with BDeaver, which is a SQL client software application and a database administration tool.\\
The project was developed using the GIT version control system, integrated with the hosting system GitHub. These two tools allow to store, modify and share the project's code, as well as to keep a records
of the activities and modifications done.\\
The data retrieving was carried out thanks to the Microsoft Graph API, which is a REST API that allows to retrieve data from Microsoft Azure.\\
Finally, the report creation was done using Power Bi, a Microsoft tool that allows to create reports in a simple and intuitive way.

\newpage
\subsection{Standards}
The whole Python coding part of the project was written following the \href{https://peps.python.org/pep-0008/}{PEP8} Style Guide for Python Code.
This guide gives coding conventions for the Python code comprising the standard library in the main Python distribution.\\
The application was developed from scratch, so I had to organize the files in order to maintain a precise schema. In particular mine was a command-line application composed by a main file, called \texttt{console.py}
and several other internal packages. Following the \href{https://docs.python-guide.org/writing/structure/}{standard layout}, I divided all my files into subfolders, each one containing packages divided with
respect to their use and their functionality.\\
Throughout the developing of my project, I made an extensive use of the Docker technology, which allowed me to create a containerized environment, isolated from the rest of the system,
in which the software was developed, tested and run.

\subsection{Training}
During my internship experience I was able to learn several new technologies, I indeed spent the first week of my internship learning everything I would have needed for the developing of the project.\\
First I had to learn Python programming language, to which I was totally new. This task was carried out following different tutorials and thanks to the constant support of company tutor, who assisted me in
my training. The learning of this language was not very difficult, since, thanks to the courses I followed, I already had a strong coding background.\\
Then I start to discover the API world, particularly I had to learn how to make REST HTTP request to an endpoint in Python. Fortunately, I had already studied the basics of the HTTP protocol and the REST architecture
during the course of \emph{Introduction to databases}, so I only had to adapt my previous knowledge to the Python language.\\
Finally yet importantly, I learned, how to manage my progresses using the version control system GIT. It was the first time I used it in a working environment, but thanks to the course of \emph{Object oriented programming},
I already had experience with the SVN version control system.

\newpage
\section{Project}
\subsection{Console menu}
The main file and the only one which was effectively run, is the \texttt{console.py} file. This file is composed by a main menu, which allows to choose the desired command.\\
The menu allows the user to choose which command to run or to open a help menu, which provide the user with information about the available commands and their usage. The selected command has to be passed as a
parameter when the script is run, some commands may accept optional parameters as indicated in the help menu. If no command is specified, the default action is to open the help menu.
\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth, height=7cm]{help-menu.png}
    \caption{help menu}
\end{figure}\\
Summing up, the main role of the console is to import all the required packages and execute the commands, through all the function contained in the other files.

\subsection{Configuration}
The parameters needed in the project, such as database information, API URLs, JSON files and other data files, are stored in the \texttt{config.ini} file. This allows the user to have a much more clear
overview of the possibility to change the information based on the current needs.\\
On the other hand this data are not directly contained in the Python code, so they need to be retrieved. The \texttt{ConfigParser} package comes in our help, allowing as to retrieve the data from the config file
and return them in any Python function.
\begin{python}
    def get_url():
        parser = ConfigParser()
        parser.read("config.ini")
        url = parser.get('ping', 'url')
        return url
\end{python}
The above code is an example of how the configuration function works. In the example, the parameter \texttt{url} is retrieved from the section \texttt{ping} of the \texttt{config.ini} file.

\subsection{Database Connection}
The first important step of the project was to set up a database. As said in the previous sections, the database was a PostgreSQL database, hosted in the Azure cloud.\\
One the database was up and running, the next step was to create a connection between the Python application and the database, in order to be able to store, modify and delete data directly from the application.\\
It twas chosen to use the \texttt{Psycopg2} library, which is the most popular PostgreSQL database adapter for the Python programming language. This package allows to open a connection, query the data, 
commit the changes and finally close the connection.
\begin{python}
    # parameters are host, database, user, password
    def connect(params):
        conn = None
        # connect to the PostgreSQL server
        conn = psycopg2.connect(**params)
        # create a cursor
        cur = conn.cursor()
        # execute a statement
        cur.execute('SELECT version()')
        # display the PostgreSQL database server version
        db_version = cur.fetchone()
        print(db_version)
        # close the communication with the PostgreSQL
        cur.close()
        return conn
\end{python}
The above code describe the \texttt{connect} function, that is one of the most fundamental functions of the project. It opens a connection with the database, then the \texttt{cursor} object allows to execute a query
and retrieve the result. Finally, the cursor is closed and the \texttt{conn} object is returned, in order to be used wherever is needed a database connection.

\subsection{Migrations}
\label{subsec:migrations}
During the development of the project the database was constantly modified: some tables was added, removed or altered. In order to keep track of all the changes, it was created a table named \texttt{migrations},
used to store the changes made to the database. The table is composed of two columns: the migration version and exact date and time of the entry insertion.\\
The concept of migrations was widely known in the company IT department, where the team makes a intense use of the \href{https://www.doctrine-project.org/projects/doctrine-migrations/en/3.3/index.html}{migration doctrine}, 
applied to PHP. In order to aligne mt project with the company standards, it was chosen to use the concept of migrations. \\
There exist Python libraries useful to manage all the migrations, but since this is a small project, instead of learn the use of a complex library, it was chosen to personally develop the functions needed for the management 
of the database migrations.
\begin{figure}[h]
    \centering
    \includegraphics[width=8cm, height=4cm]{table-migrations.png}
    \caption{table migrations}
\end{figure}\\
Each time it was needed to modify the database, a SQL file was written and named with current date and time. Then it was checked if the instruction written into the file were ready to be applied to the 
database.\\
First it was checked if the name of the migration file respects the naming constraints, i.e., thanks to the \texttt{datetime} library, it was checked if the name was a valid date and time format.
The next step was to find out if the file was not already present in the database and if it was the most recent. If both conditions were verified, then the file was inserted in the \emph{ready to be executed}
list, meaning that the migrations was ready to be applied to the database.
\newline\newline\newline\newline
The main menu offers two migrations command:
\begin{itemize}
    \item \texttt{db-mig-status}: allows the user to have an overview of the migrations' status. It lists the migrations ready to be executed, the ones available only in the file system and those available
            only in the database. The last two are indicated as errors, since the file systems and the database migrations table must be aligned.
    \item \texttt{db-mig-execute}: execute all the migrations labelled as \emph{ready to be executed}. If the execution is successful the changes are committed to the database and a new entry is inserted in the
            migrations table. If the execution is not successful, the changes are rolled back and the user is informed.
\end{itemize}

\subsection{Azure Application}
\subsubsection{Azure Environment}
Azure is a cloud computing based platform for hosting applications and website, created and operated by Microsoft. After registering an application in the Azure portal, one can access and use all the services
provided by Microsoft Graph.
\subsubsection{Application Registration}
The project objective is to retrieve and analyze the data from Microsoft Graph. In order to be able to use this service, I had to register my application in the Azure portal.\\
Following the \href{https://docs.microsoft.com/en-us/graph/auth-v2-service#authentication-and-authorization-steps}{official documentation} and thanks to my company tutor support, the application was correctly
registered and the required permission was given. After the registration procedure, the application details were generated by the portal: client ID, secret, authority and scope. This data was saved in the 
\texttt{config.ini} file since they would be used soon to access the Microsoft Graph API.

\subsection{Microsoft Graph}
\subsubsection{Authorization and Connection}
Once the application was registered in Azure platform, in order to access the Microsoft Graph API, I had to connect my Python code to Microsoft Graph API. \\
Microsoft Azure makes use of \href{https://oauth.net/2/}{OAuth 2.0}, which is the industry-standard protocol for authorization, based on the concept of access tokens.\\
Authorization and connection was implemented following the \href{https://github.com/Azure-Samples/ms-identity-python-daemon/tree/master/1-Call-MsGraph-WithSecret}{official guide} and using the \texttt{msal} package.\\
Once the connection was established, an authentication token was generated, which is later used to access the Microsoft Graph API.
\begin{python}
  def connect(authority, client_id, scope, secret):
      app = msal.ConfidentialClientApplication(
            client_id, client_credential=secret, 
            authority=authority)
      result = app.acquire_token_silent(scope, account=None)
      if not result:
          result = app.acquire_token_for_client(scopes=scope)
      return result['access_token']
\end{python}
The above code snippet explains how the connection is established. The needed parameters are the client authority, ID, scope and secret. Then is checked if the token is still valid. If it is not, it is renewed
and at the end the token is returned and ready to be used to perform the requests to the Microsoft Graph API.

\subsubsection{Graph: Users}
The first set of data which I worked with is the list of all the users registered with a company Microsoft account.

\paragraph{Data Retrieval} ~\\
The first step was to retrieve the list of users from the Microsoft Graph API. The task was performed, as explained before, with a GET request, through the \texttt{requests} library.\\
An endpoint was needed and the one I used had the following url:  
\begin{Verbatim}[fontsize=\small]
https://graph.microsoft.com/v1.0/users?$top=999&$select=id,displayName
\end{Verbatim}
where the \texttt{\$select} parameter is used to retrieve only the fields needed.\\
The request is done inserting in the header the authorization token, whose importance was explained before. Finally, if the request was successful, the list of users is returned.

\paragraph{Response Validation} ~\\
Once the response of the endpoint was available, it needs to be validate, i.e., it must be checked that all the required fields are present and in the correct format.\\
The response of every API is a \texttt{json} text, so it is checked against a \texttt{json} file specifically writte for the used endpoint. This is performed using the \texttt{validate} module of the 
\texttt{jsonschema} library.
The following snippet reports the \texttt{json} file written to check each object of response of the users endpoint:
\begin{python}
    {
    "id": {"type": "string"},
    "displayName": {"type": "string"},
    "required" : ["id"]
    }
\end{python}
It states that two fields are expected: \texttt{id} and \texttt{displayName} and both must be of string type, i.e., they must be a text. Moreover, the \texttt{required} instruction imposes that the \texttt{id} field
must be present, othterwise the validation will fail, while the \texttt{displayname} may be absent.\\
This \texttt{required} instruction was specified for the \texttt{id} since it will later be used a primary key in the database, so it cannot be a null value.

\paragraph{Database Insertion} ~\\
Finally, after the data was retrieved and validated, they are ready to be inserted in the database.\\
Through a migration (see section \hyperref[subsec:migrations]{4.4} for furhter information) the table \texttt{users} was created, with the following characteristics:
\begin{figure}[h]
    \centering
    \includegraphics[width=10cm, height=2cm]{table-users.png}
    \caption{table users}
\end{figure}\\

One the users list is available, it must be inserted in the database. This is done though a cycle and for each entry of the list there are two possiblities:
\begin{itemize}
    \item \texttt{insert}. This command insert a user in the table if and only if the user is not already present in the table.
    \item \texttt{update}. If the user is already present in the table, this command updates the user's information.
\end{itemize}
FInally, at the end of the cycle, the changes to the database are committed and the connection is closed.

\newpage
\subsubsection{Graph Devices}
The second big set of data used in the project was the list of the company devices given to the employees.

\paragraph{Data Retrieval} ~\\
Such as for the users, first step was to retrieve the list of devices from the Microsoft Graph API. The task was performed, with a GET request, through the \texttt{requests} library.\\
An endpoint was needed and the one I used had the following url:  
\begin{Verbatim}[fontsize=\small]
https://graph.microsoft.com/v1.0/devices?$top=999&$select=deviceID,
id,displayName,registrationDateTime,approximateLastSignInDateTime
\end{Verbatim}
where again the \texttt{\$select} parameter is used to retrieve only the fields needed.

\paragraph{Data Validation} ~\\
In the same way as it was explained before, the response of the endpoint was validated, i.e., checked against a \texttt{json} file specifically written for the used endpoint.\\
The following piece of code shows an extract of the \texttt{json} file:
\begin{python}
{
...
"registrationDateTime": {"type": "DateTimeOffset"},
"approximateLastSignInDateTime": {"type": "DateTimeOffset"},
"required" : ["deviceId", "id", "displayName"]
}
\end{python}
The deviceID, id and displayname fileds are omitted since they are \texttt{string} and there are no differences with respect to what explained for the user \texttt{json} file.
What is new are the \texttt{registrationDateTime} and \\ \texttt{approximateLastSignInDateTime} fields, which are dates and are expressed as \texttt{DateTimeOffset} format.

\paragraph{Active Devices} ~\\
The problem of the devices list retrieved from Microsoft Graph is that it contains not onlt the active devices, but also the devices that are not active anymore, such as old notebooks or pohones.
In order to overcome this problem, the devices list was filtered to obtain only the devices effectively active in a time period, determined in the \texttt{config.ini} file.\\
After the validation the two date fields were converted in \texttt{date object}, foundamental for working with dates in Python. Then, the devices list was filtered to obtain only the devices that are active 
in the deired period.\\
The following code explains how the list of the active devices is obtained:
\begin{python}
def list_active_devices(devices, active_period):
   today = datetime.now()
   start_date = (
      today -
      timedelta(
         days=active_period,
         hours=today.hour,
         minutes=today.minute,
         seconds=today.second)).strftime("%Y-%m-%dT%H:%M:%SZ")
   active_devices = []
   for device in devices:
       last_login = device['approximateLastSignInDateTime']
       if last_login is not None and last_login >= start_date:
           active_devices.append(device)
   return active_devices
\end{python}
First the start date is found, subtracting the number of days specified in the period from the current date, thanks to the \texttt{timedelta} function. The obtained date is converted in a date object using the
\texttt{strftime} function.\\
Then, the list of devices is cycled and only the devices with a last login more recent than the start date are added to the list of active devices, that is in the end returned.\\
At the end of the function, we will have a list containing only the devices with the last login done in a defined time period.

\paragraph{Database insertion and deletion} ~\\
Finally, the list of active devices is inserted in the database. The process is similar to the one for the users.\\
The table \texttt{devices} was created, with the following characteristics:
\begin{figure}[h]
    \centering
    \includegraphics[width=12cm, height=3cm]{table-devices.png}
    \caption{table devices}
\end{figure}\\
The main difference with respect to the users part is that the devices stores in the database must be deletedonce they exit the active period.
\newline\newline
This simple piece of code solve the problem:
\begin{python}
    for device in devices:
        ...
        cur.execute(sql_check, (deviceID,))
        if cur.fetchone()[0] is False:
            insert(cur, deviceID, id, name, date, last_login)
        else:
            update(cur, deviceID, last_login)
    delete_devices(cur, active_period)
    ...
\end{python}
It checks, for each device in the list, if it is already present in the database. If it is not present, it is inserted, otherwise the device information are updated. At the end of the cycle, 
the devices that are not active anymore are deleted.

\subsubsection{Graph: Logs}
The most important and complex set of data was the list of all the logs done by every employee.

\paragraph{Data Retrieval} ~\\
The most complicated task of this part of the project was to retrieve the list of all logs from Microsoft Azure, since Graph allows to retrieve at most 1000 entry at single HTTP request. To overcome this problem
successive requestes was made. The paging system of Azure was managed included the authentication token for the next request in the header of the previous API response, when the next token field was missing it means
that it was the last page.\\
Moreover it was asked to retrieve only the logs done in a specified period, whose values was specified in the \texttt{config.ini} file.
\begin{python}
    ...
    while next_link is not None:
        print("Retrieving information from Azure")
        logs = requests.get(next_link, headers=headers).json()
        insert_logs(conn, logs['value'], file_schema)
        if "@odata.nextLink" in logs:
            next_link = logs['@odata.nextLink']
        else:
            next_link = None
    ...
\end{python}
The above code reports how the paging issue was solved: each time a page is returned, the \texttt{nextLink} is saved and used as URL for the next request.

\paragraph{Data Validation} ~\\
In the same way as explained in the preceding sections, the response of the endpoint was validated, i.e., checked against a \texttt{json} file specifically written for the used endpoint.\\
After the validation, all the date fiels were converted in \texttt{date object}, in order to be ready to be used and inserted in the database.

\paragraph{Database insertion} ~\\
FIrst the table logs was created, with the following structure:
\begin{figure}[h]
    \centering
    \includegraphics[width=13cm, height=5.5cm]{table-logs.png}
    \caption{table logs}
\end{figure}\\
Since the logs were retrieved up to a defined number of days and the script may be run everyday, only the new logs had to be inserted in the database. To achieve this result, each entry of the retrieved list 
was checked against the rows of the db. If the record is not found in the database it is inserted, otherwise it is ignored.

\subsection{Docker}
After all the code development described in the previous sections, the script was ready and able to retrieve, validate and insert the data in the database.\\
At this point the only problem was that the project was developed on a specific Windows machine, so every command needed to be executed from that machine in order to be able to run properly.\\
This was a problem since the script needed to be run from any machines, including servers anche Unix environments. To overcome this issue, the whole project was packaged in a Docker container, which
create an environment that could be run in every device.\\
First I had to learn how to use the Docker technology, then I installed Docker Desktop, which is a graphic interface for create and managing docker container. Once I was ready, I created a docker container for
my project and I wrote the needed \texttt{Dockerfile}. \\
The \texttt{Dockerfile} is the instruction file needed to correctly setup the Docker environment and inside the following parameters was specified:
\begin{itemize}
    \item \textbf{Python}: it was specified which version of Python to use and the standard path of Python in a generic machine.
    \item \textbf{Packages}: the needed packaged for the project were listed in ordere to be installed inside the Docker container.
    \item \textbf{Entrypoint}: using bash instructions, there were specified which commands to execute when the container is started.
\end{itemize}
Once the container was ready, it was possible to run the wanted script from every device.\\
In our case, the commads specified in the \texttt{Entrypoint} were the ones to retrieve, validate and insert aal the needed data in the database. At the end only one command was run:
\begin{center}
\scriptsize
\begin{BVerbatim}
docker run --rm -it --name python-dev-container -p 1010:1010 python-dev-image  
\end{BVerbatim}
\end{center}
as a conseguence of this command the database was populated with the users, devices and logs data.









\subsection{Data Analysis}
\subsection{Report Creation}
\section{Conclusion and Considerations}
\end{document}